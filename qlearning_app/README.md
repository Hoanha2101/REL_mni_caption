# âœ‚ï¸ Solving the 2D Cutting Stock Problem using Q-Learning

This repository demonstrates how **Q-Learning**, a classic reinforcement learning algorithm, can be applied to the **2D Cutting Stock Problem (2D-CSP)** to optimize material usage, reduce trim loss, and minimize the number of stock sheets used.

---

## ğŸ“¦ Problem Description

The **2D Cutting Stock Problem** involves cutting rectangular products from large rectangular stock sheets. The objectives are:

- Minimize **trim loss** (unused space).
- Fulfill all **product demands**.
- Reduce the **number of sheets used**.

Traditional optimization techniques struggle in large search spaces. We instead explore **Q-Learning**, allowing an agent to learn cutting strategies by interacting with a simulated environment.

---

## ğŸ¤– Q-Learning Overview

Q-Learning is an off-policy reinforcement learning algorithm that learns an optimal action-value function (Q-table).

### ğŸ”‘ Key Concepts

- **Q-table**: A matrix \( Q(s, a) \) estimating the expected reward for taking action `a` in state `s`.
- **Epsilon-greedy**: Balances exploration (random actions) and exploitation (greedy actions).
- **Bellman Update**:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$

---

## ğŸ§ª Cutting Stock Environment

The environment is built using the **Gymnasium** interface and includes:

### ğŸ”¹ Observation Space

- Each stock is a 2D grid:
  - `-2`: Out of bounds.
  - `-1`: Empty cell.
  - `>= 0`: Occupied by a product ID.
- Product list: sizes and quantities remaining.

### ğŸ”¹ Action Space

Each action is a dictionary:
```python
{
  "stock_idx": int,
  "size": (w, h),
  "position": (x, y)
}
```

## ğŸ”¹ Step Function & Termination

- Validates action â†’ applies cut if valid â†’ reduces product quantity.
- **Episode terminates** when all product quantities reach zero (fully cut).

---

## ğŸ”¹ Reward Function

Custom reward is calculated using:

- âœ… **Filled Ratio** â€“ Encourages placing more products in used space.
- âœ… **Bonus for Unused Stocks** â€“ Incentivizes using fewer sheets.
- âŒ **Trim Loss** â€“ Penalizes leftover space in partially used stocks.

**Total Reward = FilledRatio + UnusedStockBonus âˆ’ TrimLoss**

---

## ğŸ§  Q-Learning Agent

### ğŸ§© State Encoding

To reduce Q-table size, we extract simple features:

- `empty_space` = number of `-1` cells (free space).
- `remaining_products` = sum of product quantities left.

Then combine into a single state index:

```python
state = (empty_space * 1000 + remaining_products) % state_size
```

### ğŸ® Action Mapping

Each action index maps to:

- A **product index** to cut.
- A **placement position (x, y)** on a selected stock sheet.

âš ï¸ **Invalid actions** (e.g., overlapping or out-of-bound cuts) are **skipped** or result in a **no-operation (no-op)**.

---

### ğŸ” Learning Updates

The Q-learning agent follows the standard update rule based on the Bellman equation:

- **Learning rate (Î±)**: Controls how much newly acquired information overrides the old Q-value.
- **Discount factor (Î³)**: Determines the importance of future rewards relative to immediate rewards.
- **Epsilon decay**: Gradually reduces the probability of random action selection (exploration) over time, favoring learned actions (exploitation).
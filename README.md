
# ðŸ§© 2D Cutting Stock Problem Solver (PPO, Q-Learning & Heuristic Approaches)

This repository presents a comprehensive solution to the **2D Cutting Stock Problem (2D-CSP)** using both **Reinforcement Learning** (PPO, Q-Learning) and **Heuristic algorithms**. The project focuses on optimizing material usage in industrial settings (e.g., glass, wood, metal cutting), reducing waste, and minimizing the number of stock sheets used.

---

## ðŸ“Œ Problem Description

In manufacturing industries, large material sheets must be cut into smaller rectangles to meet demand. The challenge lies in:

- Reducing **total trim loss** (wasted material).
- Minimizing **number of stock sheets used**.
- Ensuring **feasible, non-overlapping placements**.

The project models this as a **2D-CSP** with strict constraints:
- Demand satisfaction.
- Material utilization within boundaries.
- No overlapping placements.
- Cut pieces must start from sheet edges.
- Fixed orientations (no rotation).

---

## ðŸ§  Approaches

### 1. ðŸ”§ Heuristic Algorithms

We implemented and compared three classic heuristics:

| Algorithm       | Description                                                                 | Pros                        | Cons                         |
|----------------|-----------------------------------------------------------------------------|-----------------------------|------------------------------|
| **First-Fit**   | Places each item in the first stock where it fits.                         | Fast, easy to implement     | May leave large unused gaps |
| **Best-Fit**    | Places each item where it leaves the least remaining space.                | Better utilization          | Slower than First-Fit       |
| **Combination** | Combines First-Fit and Best-Fit + stock merging optimization.              | Balanced performance        | Slightly more complex       |

> âœ… See implementation in: `first_fit_policy`, `best_fit_policy`, and `combination_policy`

---

### 2. ðŸ¤– Reinforcement Learning - PPO

We implemented a **Proximal Policy Optimization (PPO)** agent using a custom Gym environment (`CuttingGlassEnv`) to learn cutting strategies over time.

#### âš™ PPO Architecture
- **Actor-Critic model**
- **CNN** for spatial feature extraction from stocks & valid mask
- **MLP** for encoding product information
- **Actor network** predicts:
  - Product to cut
  - Cutting position (x, y)
  - Orientation (fixed in this project)
- **Critic network** estimates state-value \( V(s) \)

#### ðŸ“ˆ Reward Function

The custom reward encourages:
- High **filled ratio**.
- Low **trim loss**.
- Bonus for **unused stock sheets**.

#### âš  PPO Limitations Observed
- Slower convergence.
- May require more fine-tuning.
- Sometimes underperforms heuristics on small datasets.

---

### 3. ðŸ§  Reinforcement Learning - Q-Learning

In addition to PPO, we implemented a **Q-Learning** agent to compare value-based learning approaches in discrete action-state spaces.

#### ðŸ“Œ Key Concepts

- **Q-table**: Stores expected rewards \( Q(s, a) \) for each state-action pair.
- **Epsilon-greedy** strategy for balancing exploration and exploitation.
- **Tabular updates** using Bellman Equation:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$


#### ðŸ”¢ State Encoding

Simplified representation using:
- `empty_space`: Total available space.
- `remaining_products`: Remaining product quantity.

> Combined as: `state = (empty_space * 1000 + remaining_products) % state_size`

#### ðŸŽ® Action Mapping

Each action is an index mapped to:
- Product index
- Cutting position (x, y)

Invalid actions are skipped or retried.

#### ðŸ† Reward Strategy

| Reward Component         | Description                                              |
|--------------------------|----------------------------------------------------------|
| **Filled Ratio**         | Positive reward for high area usage                      |
| **Trim Loss**            | Penalized for unused space                               |
| **Unused Stock Bonus**   | Encourages fewer sheets used                             |
| **Final Completion Bonus** | Extra reward when task completed                        |

#### ðŸ“‰ Limitations

- May lose spatial info in simple encoding.
- Less flexible in complex environments.
- Requires good hyperparameter tuning.

---

## ðŸ“ Repository Structure

```

ðŸ“¦
â”œâ”€â”€ ðŸ“„ data_check.py
â”œâ”€â”€ ðŸ“„ gen_data.py
â”œâ”€â”€ ðŸ“„ LICENSE
â”œâ”€â”€ ðŸ“„ README.md
â”œâ”€â”€ ðŸ“„ requirements.txt
â”œâ”€â”€ ðŸ“„ show_report_5_method.py
â”œâ”€â”€ ðŸ“„ visualization.py
â”‚
â”œâ”€â”€ ðŸ“ 1_slide
â”‚   â””â”€â”€ ðŸ“„ AI17C_Group3_Project.pptx
â”‚
â”œâ”€â”€ ðŸ“ data
â”‚   â”œâ”€â”€ ðŸ“„ data_custom.csv
â”‚   â”œâ”€â”€ ðŸ“„ policy_comparison_results_5_method.csv
â”‚   â””â”€â”€ ðŸ“Š stock_product_analysis.png
â”‚
â”œâ”€â”€ ðŸ“ documentation
|   â”œâ”€â”€ ðŸ“„ BartoSutton.pdf
|   â””â”€â”€ ðŸ“„ Michael T. Goodrich, Roberto Tamassia, Michael H. Goldwasser-Data Structures and Algorithms in Python-Wiley (2013).pdf
â”‚
â”œâ”€â”€ ðŸ“ heuristic_app
â”‚   â”œâ”€â”€ ðŸ“„ README.md
â”‚   â”œâ”€â”€ ðŸ“„ report_heuristic.py
â”‚   â”œâ”€â”€ ðŸ“„ show_report_heuristic.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ data
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ data_custom.csv
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ policy_comparison_results.csv
â”‚   â”‚   â””â”€â”€ ðŸ“Š stock_product_analysis.png
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ env
â”‚   â”‚   â””â”€â”€ ðŸ§  cutting_stock.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“ policy
â”‚   â”‚   â”œâ”€â”€ âš™ï¸ BestFit_Policy.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸ Combination_Policy.py
â”‚   â”‚   â””â”€â”€ âš™ï¸ FirstFit_Policy.py
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ“ results
â”‚       â”œâ”€â”€ ðŸ“Š comparison.png
â”‚       â””â”€â”€ ðŸ“Š comparison_5_method.png
â”‚
â”œâ”€â”€ ðŸ“ ppo_app
â”‚   â”œâ”€â”€ ðŸ§  cutting_glass_env.py
â”‚   â”œâ”€â”€ ðŸ“„ main.py
â”‚   â”œâ”€â”€ ðŸ¤– ppo_agent.py
â”‚   â”œâ”€â”€ ðŸ§  ppo_policy.py
â”‚   â”œâ”€â”€ ðŸ“„ README.md
â”‚   â”œâ”€â”€ ðŸ§ª test.py
â”‚   â””â”€â”€ ðŸ“Š visualize.py
â”‚
â”‚   â”œâ”€â”€ ðŸ“ Loss_Plots
â”‚   â”‚   â”œâ”€â”€ ðŸ“ˆ loss_plot_ep21000.png
â”‚   â”‚   â””â”€â”€ ðŸ“„ README.md
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ“ Model
â”‚       â”œâ”€â”€ ðŸ’¾ ppo_policy_ep21000.pth
â”‚       â””â”€â”€ ðŸ“„ README.md
â”‚
â”œâ”€â”€ ðŸ“ qlearning_app
â”‚   â”œâ”€â”€ ðŸ“Š evaluation_all_batches.png
â”‚   â”œâ”€â”€ ðŸ“„ metrics.csv
â”‚   â”œâ”€â”€ ðŸ“Š metrics.png
â”‚   â”œâ”€â”€ ðŸ“„ README.md
â”‚
â”‚   â”œâ”€â”€ ðŸ“ agents
â”‚   â”‚   â”œâ”€â”€ ðŸ¤– dqn_agent.py
â”‚   â”‚   â”œâ”€â”€ ðŸ§  q_learning_agent.py
â”‚   â”‚   â””â”€â”€ ðŸ“„ __init__.py
â”‚
â”‚   â”œâ”€â”€ ðŸ“ data
â”‚   â”‚   â”œâ”€â”€ ðŸ“„ data_custom.csv
â”‚   â”‚   â””â”€â”€ ðŸ“„ static_data.py
â”‚
â”‚   â”œâ”€â”€ ðŸ“ env
â”‚   â”‚   â”œâ”€â”€ ðŸ§  CuttingStockEnv.py
â”‚   â”‚   â”œâ”€â”€ ðŸ§  CuttingStockEnvOptimized.py
â”‚   â”‚   â””â”€â”€ ðŸ“„ __init__.py
â”‚
â”‚   â”œâ”€â”€ ðŸ“ evaluation
â”‚   â”‚   â”œâ”€â”€ ðŸ“Š evaluate.py
â”‚   â”‚   â””â”€â”€ ðŸ“„ __init__.py
â”‚
â”‚   â””â”€â”€ ðŸ“ training
â”‚       â””â”€â”€ ðŸ‹ï¸ train_q_learning.py
â”‚
â”œâ”€â”€ ðŸ“ Report_Cutting_Stock_2D
|   â””â”€â”€REPORT_GROUP3_3_23_2025_FPT.pdf
â”‚
â””â”€â”€ ðŸ“ results
    â””â”€â”€ ðŸ“Š comparison_5_method.png

```

---

## ðŸ“Š Evaluation Metrics

| Metric               | Description                                               |
|----------------------|-----------------------------------------------------------|
| **Runtime (s)**       | Time taken for each approach                             |
| **Total Trim Loss**   | Sum of unused areas in used sheets                       |
| **Used Stocks**       | Number of sheets used to fulfill demand                  |
| **Remaining Stocks**  | Unused stock count                                       |
| **Avg Used Stock Area** | Avg area covered in each used sheet                    |

---

## ðŸ“ˆ Comparison Summary


| Method       | Trim Loss â†“             | Used Stocks â†“         | Speed â†‘     | Learning Capability â†‘ |
|--------------|--------------------------|------------------------|-------------|------------------------|
| **First-Fit**   | âš ï¸ Medium               | âœ… Low                 | âœ…âœ…âœ…       | âŒ                    |
| **Best-Fit**    | âœ… Lowest               | âš ï¸ Slightly High       | âœ…           | âŒ                    |
| **Combination** | âœ… Balanced             | âœ… Balanced            | âœ…âœ…         | âŒ                    |
| **PPO**         | âŒ High                 | âŒ High                | âš ï¸ Slow      | âœ…âœ…âœ…                 |
| **Q-Learning**  | âš ï¸ Unstable (varies)    | âš ï¸ Inconsistent        | âœ…âœ…âœ…       | âœ…âœ…                   |

---

## ðŸš€ Getting Started

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Use heuristic
```bash
python heuristic_app/report_heuristic.py
python heuristic_app/show_report_heuristic.py
```

### Train PPO Agent
```bash
python ppo_app/main.py
```

### Train Q-learning Agent
```bash
python qlearning_app/training/train_q_learning.py
```


### Visualize Performance
```bash
python show_report_5_method.py
```

---
## ðŸ“šResults
![Alt text](results/comparison_5_method.png)

For 2D material cutting problems, heuristics are still a reliable baseline in real production because of their stable performance and fast speed.

However, Reinforcement Learning is a promising direction for automated intelligent systems, especially when the problem is large and complex.

Combining heuristics + RL or fine-tuning policy after training is a potential trend.

---

## ðŸ“š References

- [Environment](https://github.com/martinakaduc/gym-cutting-stock.git)
- [ðŸ“˜ PPO Paper (Schulman et al., 2017)](https://arxiv.org/pdf/1707.06347)
- [ðŸ“ PPO Medium Article](https://medium.com/@oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200)
- [ðŸŽ¥ PPO Tutorial on YouTube](https://www.youtube.com/watch?v=hlv79rcHws0)

---

## ðŸ‘¥ Authors

> **FPT University â€“ Quy Nhon AI Campus**  
> Faculty of Information Technology â€“ Capstone Project â€“ Mar 2025

- Ha Khai Hoan â€“ QE170157  
- Dang Phuc Bao Chau â€“ QE170060  
- Nguyen Van Sy Thinh â€“ SE173018  
- Nguyen Van Thu â€“ QE170147  
- Nguyen Quoc Vuong â€“ QE170168  

**Instructor:** Dr. Nguyen An Khuong

---

## ðŸ“„ License

MIT License â€“ See `LICENSE` for more details.

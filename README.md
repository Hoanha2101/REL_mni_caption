
# ğŸ§© 2D Cutting Stock Problem Solver (PPO, Q-Learning & Heuristic Approaches)

This repository presents a comprehensive solution to the **2D Cutting Stock Problem (2D-CSP)** using both **Reinforcement Learning** (PPO, Q-Learning) and **Heuristic algorithms**. The project focuses on optimizing material usage in industrial settings (e.g., glass, wood, metal cutting), reducing waste, and minimizing the number of stock sheets used.

---

## ğŸ“Œ Slide

```bash
ğŸ“¦
â”œâ”€â”€ ğŸ“ 1_slide
â”‚   â””â”€â”€ ğŸ“„ AI17C_Group3_Project.pptx
```

## ğŸ“Œ Report 

```bash
ğŸ“¦
â”œâ”€â”€ ğŸ“ Report_Cutting_Stock_2D
|   â””â”€â”€REPORT_GROUP3_3_23_2025_FPT.pdf
```

## ğŸ“Œ Problem Description

In manufacturing industries, large material sheets must be cut into smaller rectangles to meet demand. The challenge lies in:

- Reducing **total trim loss** (wasted material).
- Minimizing **number of stock sheets used**.
- Ensuring **feasible, non-overlapping placements**.

The project models this as a **2D-CSP** with strict constraints:
- Demand satisfaction.
- Material utilization within boundaries.
- No overlapping placements.
- Cut pieces must start from sheet edges.
- Fixed orientations (no rotation).

---

## ğŸ“Œ Constraints

To ensure feasibility and optimize production efficiency in the 2D Cutting Stock Problem, the following constraints are applied:

### 1. Demand Fulfillment Constraint  
**Objective**: Ensure that the number of cut pieces meets or exceeds demand for each item.  
**Expression**: 
$$ \sum_{j} (a_{ij} \cdot x_j) \geq d_i \quad \forall i $$   
- a_ij: number of item i in pattern j  
- x_j: number of times pattern j is used  
- d_i: demand of item i  

### 2. Material Utilization Constraint  
**Objective**: Ensure the total area of all cut pieces in a pattern does not exceed the raw stock area.  
**Expression**:
 $$ \sum_{i} (w_i \cdot h_i \cdot a_{ij}) \leq W \cdot H \quad \forall j $$
- w_i, h_i: width and height of item i  
- W, H: width and height of the stock material  

### 3. Non-Overlapping Constraint  
**Objective**: No two pieces in the same pattern overlap. This is handled using geometric or combinatorial "no-overlap" conditions.  

### 4. Binary and Non-Negativity Constraints  
**Objective**: Ensure logical and implementable decisions.  
- Binary decision variables: x_ij âˆˆ {0, 1}  
- Number of uses of each pattern must be non-negative: x_j â‰¥ 0  

### 5. Seamless and Starting from the Edge Constraint

To optimize material usage and simplify the cutting process, pieces must be:

- **Seamless**: placed without unnecessary gaps.
- **Starting from the edge**: the first cuts must touch the edge of the material sheet.

Assuming the sheet has size \( W \times H \), and each piece \( i \) has top-left coordinates \( (x_i, y_i) \) and dimensions \( (w_i, h_i) \), the placement constraint is:

<div align="center">

$$
\begin{cases}
x_i = 0 \quad \text{or} \quad x_i = x_j + w_j \\\\
y_i = 0 \quad \text{or} \quad y_i = y_j + h_j
\end{cases}
$$

</div>

This ensures that each piece is either aligned with the sheet edge or with another piece.

To avoid large unusable gaps between pieces, we also enforce:

<div align="center">

$$
x_{i+1} = x_i + w_i \quad \text{or} \quad y_{i+1} = y_i + h_i
$$

</div>



### 6. Orientation Constraint  
**Objective**: Treat pieces with the same dimensions but different orientations (e.g., 4Ã—2 vs. 2Ã—4) as distinct types. Orientation must follow predefined rules. Stocks and products are indexed starting from 1 (0 in code).

### 7. Orientation Constraint  
**Objective**: Stocks and products are numbered sequentially starting from 1 (index is 0 in the code) regardless of their similar size.
![Alt text](illustration/1.png)

---


## ğŸ§  Approaches

### 1. ğŸ”§ Heuristic Algorithms

We implemented and compared three classic heuristics:

| Algorithm       | Description                                                                 | Pros                        | Cons                         |
|----------------|-----------------------------------------------------------------------------|-----------------------------|------------------------------|
| **First-Fit**   | Places each item in the first stock where it fits.                         | Fast, easy to implement     | May leave large unused gaps |
| **Best-Fit**    | Places each item where it leaves the least remaining space.                | Better utilization          | Slower than First-Fit       |
| **Combination** | Combines First-Fit and Best-Fit + stock merging optimization.              | Balanced performance        | Slightly more complex       |

> âœ… See implementation in: `first_fit_policy`, `best_fit_policy`, and `combination_policy`

---

### 2. ğŸ¤– Reinforcement Learning - PPO

We implemented a **Proximal Policy Optimization (PPO)** agent using a custom Gym environment (`CuttingGlassEnv`) to learn cutting strategies over time.

#### âš™ PPO Architecture
- **Actor-Critic model**
- **CNN** for spatial feature extraction from stocks & valid mask
- **MLP** for encoding product information
- **Actor network** predicts:
  - Product to cut
  - Cutting position (x, y)
  - Orientation (fixed in this project)
- **Critic network** estimates state-value \( V(s) \)

#### ğŸ“ˆ Reward Function

The custom reward encourages:
- High **filled ratio**.
- Low **trim loss**.
- Bonus for **unused stock sheets**.

#### âš  PPO Limitations Observed
- Slower convergence.
- May require more fine-tuning.
- Sometimes underperforms heuristics on small datasets.

---

### 3. ğŸ§  Reinforcement Learning - Q-Learning

In addition to PPO, we implemented a **Q-Learning** agent to compare value-based learning approaches in discrete action-state spaces.

#### ğŸ“Œ Key Concepts

- **Q-table**: Stores expected rewards \( Q(s, a) \) for each state-action pair.
- **Epsilon-greedy** strategy for balancing exploration and exploitation.
- **Tabular updates** using Bellman Equation:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right]
$$


#### ğŸ”¢ State Encoding

Simplified representation using:
- `empty_space`: Total available space.
- `remaining_products`: Remaining product quantity.

> Combined as: `state = (empty_space * 1000 + remaining_products) % state_size`

#### ğŸ® Action Mapping

Each action is an index mapped to:
- Product index
- Cutting position (x, y)

Invalid actions are skipped or retried.

#### ğŸ† Reward Strategy

| Reward Component         | Description                                              |
|--------------------------|----------------------------------------------------------|
| **Filled Ratio**         | Positive reward for high area usage                      |
| **Trim Loss**            | Penalized for unused space                               |
| **Unused Stock Bonus**   | Encourages fewer sheets used                             |
| **Final Completion Bonus** | Extra reward when task completed                        |

#### ğŸ“‰ Limitations

- May lose spatial info in simple encoding.
- Less flexible in complex environments.
- Requires good hyperparameter tuning.

---

## ğŸ“ Repository Structure

```

ğŸ“¦
â”œâ”€â”€ ğŸ“„ data_check.py
â”œâ”€â”€ ğŸ“„ gen_data.py
â”œâ”€â”€ ğŸ“„ LICENSE
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ requirements.txt
â”œâ”€â”€ ğŸ“„ show_report_5_method.py
â”œâ”€â”€ ğŸ“„ visualization.py
â”‚
â”œâ”€â”€ ğŸ“ 1_slide
â”‚   â””â”€â”€ ğŸ“„ AI17C_Group3_Project.pptx
â”‚
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ ğŸ“„ data_custom.csv
â”‚   â”œâ”€â”€ ğŸ“„ policy_comparison_results_5_method.csv
â”‚   â””â”€â”€ ğŸ“Š stock_product_analysis.png
â”‚
â”œâ”€â”€ ğŸ“ documentation
|   â”œâ”€â”€ ğŸ“„ BartoSutton.pdf
|   â””â”€â”€ ğŸ“„ Michael T. Goodrich, Roberto Tamassia, Michael H. Goldwasser-Data Structures and Algorithms in Python-Wiley (2013).pdf
â”‚
â”œâ”€â”€ ğŸ“ heuristic_app
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ“„ report_heuristic.py
â”‚   â”œâ”€â”€ ğŸ“„ show_report_heuristic.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ data
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ data_custom.csv
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ policy_comparison_results.csv
â”‚   â”‚   â””â”€â”€ ğŸ“Š stock_product_analysis.png
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ env
â”‚   â”‚   â””â”€â”€ ğŸ§  cutting_stock.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ policy
â”‚   â”‚   â”œâ”€â”€ âš™ï¸ BestFit_Policy.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸ Combination_Policy.py
â”‚   â”‚   â””â”€â”€ âš™ï¸ FirstFit_Policy.py
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ results
â”‚       â”œâ”€â”€ ğŸ“Š comparison.png
â”‚       â””â”€â”€ ğŸ“Š comparison_5_method.png
â”‚
â”œâ”€â”€ ğŸ“ ppo_app
â”‚   â”œâ”€â”€ ğŸ§  cutting_glass_env.py
â”‚   â”œâ”€â”€ ğŸ“„ main.py
â”‚   â”œâ”€â”€ ğŸ¤– ppo_agent.py
â”‚   â”œâ”€â”€ ğŸ§  ppo_policy.py
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚   â”œâ”€â”€ ğŸ§ª test.py
â”‚   â””â”€â”€ ğŸ“Š visualize.py
â”‚
â”‚   â”œâ”€â”€ ğŸ“ Loss_Plots
â”‚   â”‚   â”œâ”€â”€ ğŸ“ˆ loss_plot_ep21000.png
â”‚   â”‚   â””â”€â”€ ğŸ“„ README.md
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ Model
â”‚       â”œâ”€â”€ ğŸ’¾ ppo_policy_ep21000.pth
â”‚       â””â”€â”€ ğŸ“„ README.md
â”‚
â”œâ”€â”€ ğŸ“ qlearning_app
â”‚   â”œâ”€â”€ ğŸ“Š evaluation_all_batches.png
â”‚   â”œâ”€â”€ ğŸ“„ metrics.csv
â”‚   â”œâ”€â”€ ğŸ“Š metrics.png
â”‚   â”œâ”€â”€ ğŸ“„ README.md
â”‚
â”‚   â”œâ”€â”€ ğŸ“ agents
â”‚   â”‚   â”œâ”€â”€ ğŸ¤– dqn_agent.py
â”‚   â”‚   â”œâ”€â”€ ğŸ§  q_learning_agent.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚
â”‚   â”œâ”€â”€ ğŸ“ data
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ data_custom.csv
â”‚   â”‚   â””â”€â”€ ğŸ“„ static_data.py
â”‚
â”‚   â”œâ”€â”€ ğŸ“ env
â”‚   â”‚   â”œâ”€â”€ ğŸ§  CuttingStockEnv.py
â”‚   â”‚   â”œâ”€â”€ ğŸ§  CuttingStockEnvOptimized.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚
â”‚   â”œâ”€â”€ ğŸ“ evaluation
â”‚   â”‚   â”œâ”€â”€ ğŸ“Š evaluate.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py
â”‚
â”‚   â””â”€â”€ ğŸ“ training
â”‚       â””â”€â”€ ğŸ‹ï¸ train_q_learning.py
â”‚
â”œâ”€â”€ ğŸ“ Report_Cutting_Stock_2D
|   â””â”€â”€REPORT_GROUP3_3_23_2025_FPT.pdf
â”‚
â””â”€â”€ ğŸ“ results
    â””â”€â”€ ğŸ“Š comparison_5_method.png

```

---

## ğŸ“Š Evaluation Metrics

| Metric               | Description                                               |
|----------------------|-----------------------------------------------------------|
| **Runtime (s)**       | Time taken for each approach                             |
| **Total Trim Loss**   | Sum of unused areas in used sheets                       |
| **Used Stocks**       | Number of sheets used to fulfill demand                  |
| **Remaining Stocks**  | Unused stock count                                       |
| **Avg Used Stock Area** | Avg area covered in each used sheet                    |

---

## ğŸ“ˆ Comparison Summary


| Method       | Trim Loss â†“             | Used Stocks â†“         | Speed â†‘     | Learning Capability â†‘ |
|--------------|--------------------------|------------------------|-------------|------------------------|
| **First-Fit**   | âš ï¸ Medium               | âœ… Low                 | âœ…âœ…âœ…       | âŒ                    |
| **Best-Fit**    | âœ… Lowest               | âš ï¸ Slightly High       | âœ…           | âŒ                    |
| **Combination** | âœ… Balanced             | âœ… Balanced            | âœ…âœ…         | âŒ                    |
| **PPO**         | âŒ High                 | âŒ High                | âš ï¸ Slow      | âœ…âœ…âœ…                 |
| **Q-Learning**  | âš ï¸ Unstable (varies)    | âš ï¸ Inconsistent        | âœ…âœ…âœ…       | âœ…âœ…                   |

---

## ğŸš€ Getting Started

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Use heuristic
```bash
python heuristic_app/report_heuristic.py
python heuristic_app/show_report_heuristic.py
```

### Train PPO Agent
```bash
python ppo_app/main.py
```

### Train Q-learning Agent
```bash
python qlearning_app/training/train_q_learning.py
```


### Visualize Performance
```bash
python show_report_5_method.py
```

---
## ğŸ“šResults
![Alt text](results/comparison_5_method.png)

For 2D material cutting problems, heuristics are still a reliable baseline in real production because of their stable performance and fast speed.

However, Reinforcement Learning is a promising direction for automated intelligent systems, especially when the problem is large and complex.

Combining heuristics + RL or fine-tuning policy after training is a potential trend.

---

## ğŸ“š References

- [Environment](https://github.com/martinakaduc/gym-cutting-stock.git)
- [ğŸ“˜ PPO Paper (Schulman et al., 2017)](https://arxiv.org/pdf/1707.06347)
- [ğŸ“ PPO Medium Article](https://medium.com/@oleglatypov/a-comprehensive-guide-to-proximal-policy-optimization-ppo-in-ai-82edab5db200)
- [ğŸ¥ PPO Tutorial on YouTube](https://www.youtube.com/watch?v=hlv79rcHws0)

---

## ğŸ‘¥ Authors

> **FPT University â€“ Quy Nhon AI Campus**  
> Faculty of Information Technology â€“ Capstone Project â€“ Mar 2025

- Ha Khai Hoan â€“ QE170157  
- Dang Phuc Bao Chau â€“ QE170060  
- Nguyen Van Sy Thinh â€“ SE173018  
- Nguyen Van Thu â€“ QE170147  
- Nguyen Quoc Vuong â€“ QE170168  

**Instructor:** Dr. Nguyen An Khuong

---

## ğŸ“„ License

MIT License â€“ See `LICENSE` for more details.
